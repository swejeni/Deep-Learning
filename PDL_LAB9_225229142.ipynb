{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDL Lab9. Image Classification using CNN for CIFAR-10 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SWETHA JENIFER\n",
    "#### 225229142"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.13.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages\n",
      "Requires: tensorflow-intel\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: numpy\n",
      "Version: 1.25.2\n",
      "Summary: Fundamental package for array computing in Python\n",
      "Home-page: https://www.numpy.org\n",
      "Author: Travis E. Oliphant et al.\n",
      "Author-email: \n",
      "License: BSD-3-Clause\n",
      "Location: c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: astropy, bkcharts, bokeh, Bottleneck, daal4py, datashader, datashape, gensim, h5py, holoviews, hvplot, imagecodecs, imageio, librosa, matplotlib, mkl-fft, mkl-random, numba, numexpr, opt-einsum, pandas, patsy, pyerfa, PyWavelets, scikit-image, scikit-learn, scikit-network, scipy, seaborn, soxr, statsmodels, tables, tensorboard, tensorflow-intel, tifffile, transformers, xarray\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (2.13.0)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.6.0 requires daal==2021.4.0, which is not installed.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.24.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.56.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: packaging in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.3.0)\n",
      "Collecting numpy<=1.24.3,>=1.22\n",
      "  Using cached numpy-1.24.3-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (63.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from packaging->tensorflow-intel==2.13.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Successfully uninstalled numpy-1.25.2\n",
      "Successfully installed numpy-1.24.3\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\sweth\\downloads\\nlp\\lib\\site-packages (1.24.3)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.25.2-cp39-cp39-win_amd64.whl (15.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "Successfully installed numpy-1.25.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.6.0 requires daal==2021.4.0, which is not installed.\n",
      "tensorflow-intel 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.25.2 which is incompatible.\n",
      "scipy 1.9.1 requires numpy<1.25.0,>=1.18.5, but you have numpy 1.25.2 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.25.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6TK3qsiOJzZc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sweth\\Downloads\\nlp\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGKQeQEQJ2gE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "vEeKKLSNKO17",
    "outputId": "739cdd7c-2062-49ba-ce38-33cd634d3c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 790s 5us/step\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(x_train,y_train),(x_test,y_test)=cifar10.load_data()\n",
    "print('x_train shape:',x_train.shape)\n",
    "print(x_train.shape[0],'train samples')\n",
    "print(x_test.shape[0],'test samples')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "-murREgGLxKd",
    "outputId": "7b515a09-321e-471c-d700-106cb7778600"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[5000].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "5xyzJY1oMeFQ",
    "outputId": "c86dd7a4-a4bf-45cd-f206-36883175f911",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[444])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "53EoSFK6MxzH",
    "outputId": "20c26737-84c3-4914-f047-f32c97a7890b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x194b8f9ab20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "omLzAwvmOTT4"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YbW9O4r9M-iv"
   },
   "outputs": [],
   "source": [
    "num_classes=10\n",
    "y_train=keras.utils.to_categorical (y_train,num_classes)\n",
    "y_test=keras.utils.to_categorical (y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "RppsasRpNiwv",
    "outputId": "3e29d78e-241e-4b3a-ec49-b13637e08059"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step : 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "CzcYoybZPhFE"
   },
   "outputs": [],
   "source": [
    "x_train=x_train.astype('float32')\n",
    "\n",
    "x_test=x_test.astype('float32')\n",
    "x_train /=255\n",
    "x_test /=255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step : 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "x5fWs7vXTsLP"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "num_classes = 10\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Define the CNN architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (5, 5), strides=(2, 2), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), strides=(2, 2), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Adam in module keras.src.optimizers.adam:\n",
      "\n",
      "class Adam(keras.src.optimizers.optimizer.Optimizer)\n",
      " |  Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, weight_decay=None, clipnorm=None, clipvalue=None, global_clipnorm=None, use_ema=False, ema_momentum=0.99, ema_overwrite_frequency=None, jit_compile=True, name='Adam', **kwargs)\n",
      " |  \n",
      " |  Optimizer that implements the Adam algorithm.\n",
      " |  \n",
      " |  Adam optimization is a stochastic gradient descent method that is based on\n",
      " |  adaptive estimation of first-order and second-order moments.\n",
      " |  \n",
      " |  According to\n",
      " |  [Kingma et al., 2014](http://arxiv.org/abs/1412.6980),\n",
      " |  the method is \"*computationally\n",
      " |  efficient, has little memory requirement, invariant to diagonal rescaling of\n",
      " |  gradients, and is well suited for problems that are large in terms of\n",
      " |  data/parameters*\".\n",
      " |  \n",
      " |  Args:\n",
      " |    learning_rate: A `tf.Tensor`, floating point value, a schedule that is a\n",
      " |      `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n",
      " |      that takes no arguments and returns the actual value to use. The\n",
      " |      learning rate. Defaults to `0.001`.\n",
      " |    beta_1: A float value or a constant float tensor, or a callable\n",
      " |      that takes no arguments and returns the actual value to use. The\n",
      " |      exponential decay rate for the 1st moment estimates. Defaults to `0.9`.\n",
      " |    beta_2: A float value or a constant float tensor, or a callable\n",
      " |      that takes no arguments and returns the actual value to use. The\n",
      " |      exponential decay rate for the 2nd moment estimates. Defaults to\n",
      " |      `0.999`.\n",
      " |    epsilon: A small constant for numerical stability. This epsilon is\n",
      " |      \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
      " |      Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n",
      " |      `1e-7`.\n",
      " |    amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from\n",
      " |      the paper \"On the Convergence of Adam and beyond\". Defaults to `False`.\n",
      " |    name: String. The name to use\n",
      " |        for momentum accumulator weights created by\n",
      " |        the optimizer.\n",
      " |    weight_decay: Float, defaults to None. If set, weight decay is applied.\n",
      " |    clipnorm: Float. If set, the gradient of each weight is individually\n",
      " |        clipped so that its norm is no higher than this value.\n",
      " |    clipvalue: Float. If set, the gradient of each weight is clipped to be no\n",
      " |        higher than this value.\n",
      " |    global_clipnorm: Float. If set, the gradient of all weights is clipped so\n",
      " |        that their global norm is no higher than this value.\n",
      " |    use_ema: Boolean, defaults to False. If True, exponential moving average\n",
      " |        (EMA) is applied. EMA consists of computing an exponential moving\n",
      " |        average of the weights of the model (as the weight values change after\n",
      " |        each training batch), and periodically overwriting the weights with\n",
      " |        their moving average.\n",
      " |    ema_momentum: Float, defaults to 0.99. Only used if `use_ema=True`.\n",
      " |        This is the momentum to use when computing\n",
      " |        the EMA of the model's weights:\n",
      " |        `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n",
      " |        current_variable_value`.\n",
      " |    ema_overwrite_frequency: Int or None, defaults to None. Only used if\n",
      " |        `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations,\n",
      " |        we overwrite the model variable by its moving average.\n",
      " |        If None, the optimizer\n",
      " |        does not overwrite model variables in the middle of training, and you\n",
      " |        need to explicitly overwrite the variables at the end of training\n",
      " |        by calling `optimizer.finalize_variable_values()`\n",
      " |        (which updates the model\n",
      " |        variables in-place). When using the built-in `fit()` training loop,\n",
      " |        this happens automatically after the last epoch,\n",
      " |        and you don't need to do anything.\n",
      " |    jit_compile: Boolean, defaults to True.\n",
      " |        If True, the optimizer will use XLA\n",
      " |        compilation. If no GPU device is found, this flag will be ignored.\n",
      " |    mesh: optional `tf.experimental.dtensor.Mesh` instance. When provided,\n",
      " |        the optimizer will be run in DTensor mode, e.g. state\n",
      " |        tracking variable will be a DVariable, and aggregation/reduction will\n",
      " |        happen in the global DTensor context.\n",
      " |    **kwargs: keyword arguments only used for backward compatibility.\n",
      " |  \n",
      " |  Reference:\n",
      " |    - [Kingma et al., 2014](http://arxiv.org/abs/1412.6980)\n",
      " |    - [Reddi et al., 2018](\n",
      " |        https://openreview.net/pdf?id=ryQu7f-RZ) for `amsgrad`.\n",
      " |  \n",
      " |  Notes:\n",
      " |  \n",
      " |  The default value of 1e-7 for epsilon might not be a good default in\n",
      " |  general. For example, when training an Inception network on ImageNet a\n",
      " |  current good choice is 1.0 or 0.1. Note that since Adam uses the\n",
      " |  formulation just before Section 2.1 of the Kingma and Ba paper rather than\n",
      " |  the formulation in Algorithm 1, the \"epsilon\" referred to here is \"epsilon\n",
      " |  hat\" in the paper.\n",
      " |  \n",
      " |  The sparse implementation of this algorithm (used when the gradient is an\n",
      " |  IndexedSlices object, typically because of `tf.gather` or an embedding\n",
      " |  lookup in the forward pass) does apply momentum to variable slices even if\n",
      " |  they were not used in the forward pass (meaning they have a gradient equal\n",
      " |  to zero). Momentum decay (beta1) is also applied to the entire momentum\n",
      " |  accumulator. This means that the sparse behavior is equivalent to the dense\n",
      " |  behavior (in contrast to some momentum implementations which ignore momentum\n",
      " |  unless a variable slice was actually used).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Adam\n",
      " |      keras.src.optimizers.optimizer.Optimizer\n",
      " |      keras.src.optimizers.optimizer._BaseOptimizer\n",
      " |      tensorflow.python.trackable.autotrackable.AutoTrackable\n",
      " |      tensorflow.python.trackable.base.Trackable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, weight_decay=None, clipnorm=None, clipvalue=None, global_clipnorm=None, use_ema=False, ema_momentum=0.99, ema_overwrite_frequency=None, jit_compile=True, name='Adam', **kwargs)\n",
      " |      Create a new Optimizer.\n",
      " |  \n",
      " |  build(self, var_list)\n",
      " |      Initialize optimizer variables.\n",
      " |      \n",
      " |      Adam optimizer has 3 types of variables: momentums, velocities and\n",
      " |      velocity_hat (only set when amsgrad is applied),\n",
      " |      \n",
      " |      Args:\n",
      " |        var_list: list of model variables to build Adam variables on.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the optimizer.\n",
      " |      \n",
      " |      An optimizer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of an optimizer.\n",
      " |      The same optimizer can be reinstantiated later\n",
      " |      (without any saved state) from this configuration.\n",
      " |      \n",
      " |      Subclass optimizer should override this method to include other\n",
      " |      hyperparameters.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  update_step(self, gradient, variable)\n",
      " |      Update step given gradient and the associated model variable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.optimizers.optimizer.Optimizer:\n",
      " |  \n",
      " |  add_variable_from_reference(self, model_variable, variable_name, shape=None, initial_value=None)\n",
      " |      Create an optimizer variable from model variable.\n",
      " |      \n",
      " |      Create an optimizer variable based on the information of model variable.\n",
      " |      For example, in SGD optimizer momemtum, for each model variable, a\n",
      " |      corresponding momemtum variable is created of the same shape and dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        model_variable: tf.Variable. The corresponding model variable to the\n",
      " |          optimizer variable to be created.\n",
      " |        variable_name: String. The name prefix of the optimizer variable to be\n",
      " |          created. The create variables name will follow the pattern\n",
      " |          `{variable_name}/{model_variable.name}`, e.g., `momemtum/dense_1`.\n",
      " |        shape: List or Tuple, defaults to None. The shape of the optimizer\n",
      " |          variable to be created. If None, the created variable will have the\n",
      " |          same shape as `model_variable`.\n",
      " |        initial_value: A Tensor, or Python object convertible to a Tensor,\n",
      " |          defaults to None. The initial value of the optimizer variable, if\n",
      " |          None, the initial value will be default to 0.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An optimizer variable.\n",
      " |  \n",
      " |  aggregate_gradients(self, grads_and_vars)\n",
      " |      Aggregate gradients on all devices.\n",
      " |      \n",
      " |      By default, we will perform reduce_sum of gradients across devices.\n",
      " |      Users can implement their own aggregation logic by overriding this\n",
      " |      method.\n",
      " |      \n",
      " |      Args:\n",
      " |        grads_and_vars: List of (gradient, variable) pairs.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of (gradient, variable) pairs.\n",
      " |  \n",
      " |  apply_gradients(self, grads_and_vars, name=None, skip_gradients_aggregation=False, **kwargs)\n",
      " |      Apply gradients to variables.\n",
      " |      \n",
      " |      Args:\n",
      " |        grads_and_vars: List of `(gradient, variable)` pairs.\n",
      " |        name: string, defaults to None. The name of the namescope to\n",
      " |          use when creating variables. If None, `self.name` will be used.\n",
      " |        skip_gradients_aggregation: If true, gradients aggregation will not be\n",
      " |          performed inside optimizer. Usually this arg is set to True when you\n",
      " |          write custom code aggregating gradients outside the optimizer.\n",
      " |        **kwargs: keyword arguments only used for backward compatibility.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Variable`, representing the current iteration.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `grads_and_vars` is malformed.\n",
      " |        RuntimeError: If called in a cross-replica context.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.optimizers.optimizer._BaseOptimizer:\n",
      " |  \n",
      " |  add_variable(self, shape, dtype=None, initializer='zeros', name=None)\n",
      " |      Create an optimizer variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        shape: A list of integers, a tuple of integers, or a 1-D Tensor of\n",
      " |          type int32. Defaults to scalar if unspecified.\n",
      " |        dtype: The DType of the optimizer variable to be created. Defaults to\n",
      " |          `tf.keras.backend.floatx` if unspecified.\n",
      " |        initializer: string or callable. Initializer instance.\n",
      " |        name: The name of the optimizer variable to be created.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An optimizer variable, in the format of tf.Variable.\n",
      " |  \n",
      " |  compute_gradients(self, loss, var_list, tape=None)\n",
      " |      Compute gradients of loss on trainable variables.\n",
      " |      \n",
      " |      Args:\n",
      " |        loss: `Tensor` or callable. If a callable, `loss` should take no\n",
      " |          arguments and return the value to minimize.\n",
      " |        var_list: list or tuple of `Variable` objects to update to minimize\n",
      " |          `loss`, or a callable returning the list or tuple of `Variable`\n",
      " |          objects. Use callable when the variable list would otherwise be\n",
      " |          incomplete before `minimize` since the variables are created at the\n",
      " |          first time `loss` is called.\n",
      " |        tape: (Optional) `tf.GradientTape`. If `loss` is provided as a\n",
      " |          `Tensor`, the tape that computed the `loss` must be provided.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of (gradient, variable) pairs. Variable is always present, but\n",
      " |        gradient can be `None`.\n",
      " |  \n",
      " |  exclude_from_weight_decay(self, var_list=None, var_names=None)\n",
      " |      Exclude variables from weight decay.\n",
      " |      \n",
      " |      This method must be called before the optimizer's `build` method is\n",
      " |      called. You can set specific variables to exclude out, or set a list of\n",
      " |      strings as the anchor words, if any of which appear in a variable's\n",
      " |      name, then the variable is excluded.\n",
      " |      \n",
      " |      Args:\n",
      " |          var_list: A list of `tf.Variable`s to exclude from weight decay.\n",
      " |          var_names: A list of strings. If any string in `var_names` appear\n",
      " |              in the model variable's name, then this model variable is\n",
      " |              excluded from weight decay. For example, `var_names=['bias']`\n",
      " |              excludes all bias variables from weight decay.\n",
      " |  \n",
      " |  finalize_variable_values(self, var_list)\n",
      " |      Set the final value of model's trainable variables.\n",
      " |      \n",
      " |      Sometimes there are some extra steps before ending the variable updates,\n",
      " |      such as overriding the model variables with its average value.\n",
      " |      \n",
      " |      Args:\n",
      " |        var_list: list of model variables.\n",
      " |  \n",
      " |  load_own_variables(self, store)\n",
      " |      Set the state of this optimizer object.\n",
      " |  \n",
      " |  minimize(self, loss, var_list, tape=None)\n",
      " |      Minimize `loss` by updating `var_list`.\n",
      " |      \n",
      " |      This method simply computes gradient using `tf.GradientTape` and calls\n",
      " |      `apply_gradients()`. If you want to process the gradient before applying\n",
      " |      then call `tf.GradientTape` and `apply_gradients()` explicitly instead\n",
      " |      of using this function.\n",
      " |      \n",
      " |      Args:\n",
      " |        loss: `Tensor` or callable. If a callable, `loss` should take no\n",
      " |          arguments and return the value to minimize.\n",
      " |        var_list: list or tuple of `Variable` objects to update to minimize\n",
      " |          `loss`, or a callable returning the list or tuple of `Variable`\n",
      " |          objects.  Use callable when the variable list would otherwise be\n",
      " |          incomplete before `minimize` since the variables are created at the\n",
      " |          first time `loss` is called.\n",
      " |        tape: (Optional) `tf.GradientTape`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        None\n",
      " |  \n",
      " |  save_own_variables(self, store)\n",
      " |      Get the state of this optimizer object.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Set the weights of the optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          weights: a list of `tf.Variable`s or numpy arrays, the target values\n",
      " |              of optimizer variables. It should have the same order as\n",
      " |              `self._variables`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.src.optimizers.optimizer._BaseOptimizer:\n",
      " |  \n",
      " |  from_config(config, custom_objects=None) from builtins.type\n",
      " |      Creates an optimizer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`, capable of instantiating the\n",
      " |      same optimizer from the config dictionary.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the output of get_config.\n",
      " |          custom_objects: A Python dictionary mapping names to additional\n",
      " |            user-defined Python objects needed to recreate this optimizer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An optimizer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.optimizers.optimizer._BaseOptimizer:\n",
      " |  \n",
      " |  variables\n",
      " |      Returns variables of this optimizer.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.src.optimizers.optimizer._BaseOptimizer:\n",
      " |  \n",
      " |  iterations\n",
      " |      The number of training steps this `optimizer` has run.\n",
      " |      \n",
      " |      By default, iterations would be incremented by one every time\n",
      " |      `apply_gradients()` is called.\n",
      " |  \n",
      " |  learning_rate\n",
      " |  \n",
      " |  lr\n",
      " |      Alias of `learning_rate()`.\n",
      " |      \n",
      " |      `lr()` is heavily called in workflows using `optimizer_v2.OptimizerV2`,\n",
      " |      so we keep it for backward compabitliy.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.trackable.autotrackable.AutoTrackable:\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.trackable.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(help(tf.keras.optimizers.Adam))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.keras.optimizers import legacy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8 : 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ia8WR0_zU0vX",
    "outputId": "0c1d05bd-e4f3-45a8-c38d-6d9c24090db6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 16, 16, 32)        2432      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 8, 8, 32)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 4, 4, 64)          18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 2, 2, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               66048     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 125002 (488.29 KB)\n",
      "Trainable params: 125002 (488.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "1407/1407 [==============================] - 26s 16ms/step - loss: 1.8581 - accuracy: 0.3092 - val_loss: 1.6098 - val_accuracy: 0.4034\n",
      "Epoch 2/15\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.5928 - accuracy: 0.4113 - val_loss: 1.5260 - val_accuracy: 0.4276\n",
      "Epoch 3/15\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.4896 - accuracy: 0.4529 - val_loss: 1.3398 - val_accuracy: 0.5088\n",
      "Epoch 4/15\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.4186 - accuracy: 0.4841 - val_loss: 1.3299 - val_accuracy: 0.5060\n",
      "Epoch 5/15\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.3674 - accuracy: 0.5069 - val_loss: 1.3518 - val_accuracy: 0.5120\n",
      "Epoch 6/15\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.3251 - accuracy: 0.5206 - val_loss: 1.2972 - val_accuracy: 0.5336\n",
      "Epoch 7/15\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2931 - accuracy: 0.5360 - val_loss: 1.1797 - val_accuracy: 0.5700\n",
      "Epoch 8/15\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2792 - accuracy: 0.5437 - val_loss: 1.1799 - val_accuracy: 0.5784\n",
      "Epoch 9/15\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2547 - accuracy: 0.5532 - val_loss: 1.1282 - val_accuracy: 0.5982\n",
      "Epoch 10/15\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.2379 - accuracy: 0.5601 - val_loss: 1.1488 - val_accuracy: 0.5834\n",
      "Epoch 11/15\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.2291 - accuracy: 0.5629 - val_loss: 1.1564 - val_accuracy: 0.5860\n",
      "Epoch 12/15\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.2157 - accuracy: 0.5740 - val_loss: 1.1445 - val_accuracy: 0.5864\n",
      "Epoch 13/15\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2033 - accuracy: 0.5760 - val_loss: 1.0950 - val_accuracy: 0.6184\n",
      "Epoch 14/15\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.1967 - accuracy: 0.5781 - val_loss: 1.1472 - val_accuracy: 0.6018\n",
      "Epoch 15/15\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.1824 - accuracy: 0.5870 - val_loss: 1.1039 - val_accuracy: 0.6170\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.1211 - accuracy: 0.6010\n",
      "Test accuracy: 0.6010000109672546\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with RMSprop optimizer and categorical_crossentropy loss\n",
    "from keras.optimizers import RMSprop\n",
    "optimizer =tf.keras.optimizers.legacy.RMSprop(learning_rate=0.0005, decay=1e-6)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part -II Model Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsHiuL2VWUSw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 16, 16, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 8, 8, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               8389120   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8782666 (33.50 MB)\n",
      "Trainable params: 8782666 (33.50 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Total Parameters: 8782666\n",
      "Epoch 1/5\n",
      "514/704 [====================>.........] - ETA: 3:50 - loss: 1.5635 - accuracy: 0.4421"
     ]
    }
   ],
   "source": [
    "# Define the more complicated CNN architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.0005, decay=1e-6)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary and parameter count\n",
    "model.summary()\n",
    "print(\"Total Parameters:\", model.count_params())\n",
    "\n",
    "# Train the model for 5 epochs\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
